# Week 08 â€” YOLO Benchmarking & Edge Vision Deployment (Hailo)

Part of the **16-Week Edge AI Engineering Bootcamp**

---

## ðŸŽ¯ Week Objective

Build a **production-style edge vision system** using:
- Raspberry Pi 5
- Hailo-8L accelerator
- YOLO object detection
- Event-based IoT messaging (MQTT)

This week emphasizes **engineering judgment**, not just model execution.

---

## ðŸ§  Core Learning Goals

- Understand YOLO model trade-offs under hardware constraints
- Deploy INT8 object detection on an edge accelerator
- Measure FPS, latency, power, and accuracy proxies
- Convert vision output into **events**, not video streams
- Make justified deployment decisions

---

## ðŸ“Š Target Metrics (Core)

| Metric | Target |
|------|------|
| YOLO FPS (Hailo) | â‰¥30 FPS |
| End-to-end latency | <100 ms |
| MQTT publish latency | <50 ms |
| Total system power | <5 W |
| Deliverable | Demo video + benchmark table |

---

## ðŸ“ Folder Structure

```text
Week_08_Hailo-YOLO-IoT/
â”œâ”€â”€ day01_model_and_compile/
â”œâ”€â”€ day02_hailo_inference/
â”œâ”€â”€ day03_messaging_mqtt/
â”œâ”€â”€ day04_system_integration/
â”œâ”€â”€ day05_benchmarking_and_demo/
â”œâ”€â”€ day06_optional_yolo5_benchmark/
â”œâ”€â”€ day07_optional_stress_test/
â””â”€â”€ README.md

---

## âœ… Day 02 â€” Hailo Inference Validation (Images â†’ Live Camera)

**Day 02 validates end-to-end YOLOv8 inference on Hailo-8L**, progressing from static image inference to real-time camera input on Raspberry Pi 5.

### What Was Accomplished

* Successfully ran **INT8 YOLOv8 inference** using a compiled `.hef` on Hailo-8L
* Implemented and debugged **Hailo NMS-by-class output decoding**
* Verified correct bounding boxes and confidence scores on test images
* Transitioned inference from images to **live Pi Camera (Picamera2)**
* Achieved **~15â€“25 FPS real-time object detection** on-device
* Confirmed HailoRT streaming, preprocessing, inference, and rendering pipeline

### Key Engineering Challenges (and Resolutions)

* **Excessive detections (1000+ boxes)**
  â†’ Root cause: misinterpreted Hailo NMS output layout
  â†’ Fixed by correct stride/count parsing and score filtering

* **Segmentation faults on exit**
  â†’ Caused by HailoRT Python teardown order
  â†’ Inference results valid; accepted as a known SDK limitation for now

* **Camera integration issues**
  â†’ Resolved by switching from legacy `libcamera-hello` to `rpicam` + `Picamera2`

### Outcome

Day 02 confirms a **production-viable edge vision pipeline**:

> Camera â†’ Preprocess â†’ Hailo NPU â†’ Postprocess â†’ Visual Output

This establishes a solid foundation for **event-driven vision** in Day 03 (MQTT), where detections become messages instead of pixels.

---

### **Day 03 â€” Event-Driven YOLOv8 Inference via MQTT**

* Integrated live YOLOv8 INT8 inference on Raspberry Pi 5 with Hailo
* Implemented MQTT publisher for edge-to-system messaging
* Separated **telemetry** (FPS, heartbeat) from **events** (detections)
* Added event gating to reduce message volume and noise
* Verified end-to-end flow from Pi camera â†’ inference â†’ desktop broker
* Implemented clean shutdown and SIGINT handling for hardware safety

---

### **Day 04 â€” System Integration & MQTT Subscribers**

* Implemented desktop-side MQTT subscribers
* Added telemetry logger, event logger, and console dashboard
* Validated wildcard topic subscriptions (`edge/+/*`)
* Demonstrated clean separation between edge and system layers
* Established observability without modifying edge inference code

---

## ðŸ§  Why today *felt* lighter (and why thatâ€™s correct)

Day03 was hard because it crossed:

* hardware
* native runtimes
* Python packaging
* networking

Day04 is where things **snap into place**:

* no hardware debugging
* no segfaults
* no race conditions
* just system composition

Thatâ€™s exactly how a good architecture feels once itâ€™s right.
